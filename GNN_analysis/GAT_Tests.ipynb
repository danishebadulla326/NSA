{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceebbbe8-de9c-4a7f-86d2-df0e85f9ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "37657006-4cd7-4978-9532-5c8551eb16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.transforms import Compose\n",
    "from torch_geometric.datasets import Amazon\n",
    "from torch_geometric.transforms.random_node_split import RandomNodeSplit\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "from IPython.display import Javascript  # Restrict height of output cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "20d8e6d6-328c-488d-9da4-6af29c8358d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name='Flickr'\n",
    "#dataset_name='Amazon'\n",
    "#dataset_name='Cora'\n",
    "#dataset_name='Citeseer'\n",
    "#dataset_name='Pubmed'\n",
    "\n",
    "\n",
    "seeds = [1234567, 12345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4e44663f-d2b3-4bc2-98e0-81ec142f6fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://docs.google.com/uc?export=download&id=1crmsTbd1-2sEXsGwa2IKnIB7Zd3TmUsy&confirm=t\n",
      "Downloading https://docs.google.com/uc?export=download&id=1join-XdvX3anJU_MLVtick7MgeAQiWIZ&confirm=t\n",
      "Downloading https://docs.google.com/uc?export=download&id=1uxIkbtg5drHTsKt-PAsZZ4_yJmgFmle9&confirm=t\n",
      "Downloading https://docs.google.com/uc?export=download&id=1htXCtuktuCW8TR8KiKfrFDAxUgekQoV7&confirm=t\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Flickr():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 500\n",
      "Number of classes: 7\n",
      "\n",
      "Data(x=[89250, 500], edge_index=[2, 899756], y=[89250], train_mask=[89250], val_mask=[89250], test_mask=[89250])\n",
      "===========================================================================================================\n",
      "Number of nodes: 89250\n",
      "Number of edges: 899756\n",
      "Average node degree: 10.08\n",
      "Number of training nodes: 77250\n",
      "Training node label rate: 0.87\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid, Flickr, Amazon\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "\n",
    "if dataset_name=='Flickr':\n",
    "    transform = Compose([\n",
    "        #NormalizeFeatures(),\n",
    "        RandomNodeSplit('train_rest',num_val = 2000, num_test = 10000)\n",
    "    ])\n",
    "    dataset = Flickr(root='data/Flickr', \\\n",
    "                     transform =transform)\n",
    "elif dataset_name=='Amazon':\n",
    "    transform = Compose([\n",
    "        #NormalizeFeatures(),\n",
    "        RandomNodeSplit('train_rest',num_val = 1000, num_test = 3000)\n",
    "    ])\n",
    "    dataset = Amazon(root='data/Amazon', name='Computers', \\\n",
    "                     transform =transform)\n",
    "\n",
    "elif dataset_name in ['Cora', 'Citeseer', 'Pubmed']:\n",
    "    # For Planetoid datasets, the standard split is already defined\n",
    "    dataset = Planetoid(root=f'data/{dataset_name}', name=dataset_name)\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('===========================================================================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e9104853-b271-4b21-95f6-60b35b4a7698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, seed, hidden_channels, out_channels, num_layers, heads=8, dropout=0.3):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.feature_vals = {}\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.dropout = dropout\n",
    "        self.layers.append(GATConv(dataset.num_features, hidden_channels, heads=heads))\n",
    "        for i in range(num_layers-2):\n",
    "            self.layers.append(GATConv(hidden_channels*heads, hidden_channels, heads=heads))\n",
    "        self.layers.append(GATConv(hidden_channels*heads, out_channels, heads=1))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            x = layer(x, edge_index)\n",
    "            if i!= (len(self.layers)-1):\n",
    "                x = F.elu(x)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            self.feature_vals['conv'+str(i)] = deepcopy(x.detach().cpu().numpy())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "224a1b2a-30f4-4409-a208-6f6d1116979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "889a1dda-8931-4581-a01d-fa1fe5450719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (layers): ModuleList(\n",
      "    (0): GATConv(500, 8, heads=8)\n",
      "    (1): GATConv(64, 8, heads=8)\n",
      "    (2): GATConv(64, 8, heads=8)\n",
      "    (3): GATConv(64, 7, heads=1)\n",
      "  )\n",
      ")\n",
      "Epoch: 001, Loss: 30.7490\n",
      "Epoch: 002, Loss: 26.3083\n",
      "Epoch: 003, Loss: 22.9751\n",
      "Epoch: 004, Loss: 25.9306\n",
      "Epoch: 005, Loss: 14.3872\n",
      "Epoch: 006, Loss: 12.2486\n",
      "Epoch: 007, Loss: 9.0220\n",
      "Epoch: 008, Loss: 8.4570\n",
      "Epoch: 009, Loss: 7.6122\n",
      "Epoch: 010, Loss: 7.1342\n",
      "Epoch: 011, Loss: 7.3789\n",
      "Epoch: 012, Loss: 6.8383\n",
      "Epoch: 013, Loss: 6.1767\n",
      "Epoch: 014, Loss: 5.4838\n",
      "Epoch: 015, Loss: 5.1888\n",
      "Epoch: 016, Loss: 4.8787\n",
      "Epoch: 017, Loss: 5.2052\n",
      "Epoch: 018, Loss: 4.4151\n",
      "Epoch: 019, Loss: 4.3354\n",
      "Epoch: 020, Loss: 3.9293\n",
      "Epoch: 021, Loss: 3.8187\n",
      "Epoch: 022, Loss: 3.9186\n",
      "Epoch: 023, Loss: 3.8463\n",
      "Epoch: 024, Loss: 3.5450\n",
      "Epoch: 025, Loss: 3.6047\n",
      "Epoch: 026, Loss: 3.7084\n",
      "Epoch: 027, Loss: 3.5196\n",
      "Epoch: 028, Loss: 3.5735\n",
      "Epoch: 029, Loss: 3.1124\n",
      "Epoch: 030, Loss: 3.0824\n",
      "Epoch: 031, Loss: 3.4738\n",
      "Epoch: 032, Loss: 3.3428\n",
      "Epoch: 033, Loss: 3.2048\n",
      "Epoch: 034, Loss: 3.2404\n",
      "Epoch: 035, Loss: 3.3234\n",
      "Epoch: 036, Loss: 3.0127\n",
      "Epoch: 037, Loss: 3.0541\n",
      "Epoch: 038, Loss: 2.7715\n",
      "Epoch: 039, Loss: 2.9845\n",
      "Epoch: 040, Loss: 2.9632\n",
      "Epoch: 041, Loss: 2.9351\n",
      "Epoch: 042, Loss: 2.8639\n",
      "Epoch: 043, Loss: 2.6952\n",
      "Epoch: 044, Loss: 2.8633\n",
      "Epoch: 045, Loss: 2.8150\n",
      "Epoch: 046, Loss: 2.5823\n",
      "Epoch: 047, Loss: 2.6818\n",
      "Epoch: 048, Loss: 2.7407\n",
      "Epoch: 049, Loss: 2.5405\n",
      "Epoch: 050, Loss: 2.6845\n",
      "Epoch: 051, Loss: 2.5934\n",
      "Epoch: 052, Loss: 2.6452\n",
      "Epoch: 053, Loss: 2.4041\n",
      "Epoch: 054, Loss: 2.5189\n",
      "Epoch: 055, Loss: 2.6691\n",
      "Epoch: 056, Loss: 2.5114\n",
      "Epoch: 057, Loss: 2.3447\n",
      "Epoch: 058, Loss: 2.4282\n",
      "Epoch: 059, Loss: 2.4269\n",
      "Epoch: 060, Loss: 2.4378\n",
      "Epoch: 061, Loss: 2.2520\n",
      "Epoch: 062, Loss: 2.2276\n",
      "Epoch: 063, Loss: 2.2221\n",
      "Epoch: 064, Loss: 2.2320\n",
      "Epoch: 065, Loss: 2.2399\n",
      "Epoch: 066, Loss: 2.3786\n",
      "Epoch: 067, Loss: 2.2454\n",
      "Epoch: 068, Loss: 2.2308\n",
      "Epoch: 069, Loss: 2.0488\n",
      "Epoch: 070, Loss: 2.1048\n",
      "Epoch: 071, Loss: 2.2074\n",
      "Epoch: 072, Loss: 2.0630\n",
      "Epoch: 073, Loss: 2.0862\n",
      "Epoch: 074, Loss: 2.0295\n",
      "Epoch: 075, Loss: 2.0676\n",
      "Epoch: 076, Loss: 2.0452\n",
      "Epoch: 077, Loss: 2.0416\n",
      "Epoch: 078, Loss: 1.9525\n",
      "Epoch: 079, Loss: 2.1278\n",
      "Epoch: 080, Loss: 1.9862\n",
      "Epoch: 081, Loss: 1.9399\n",
      "Epoch: 082, Loss: 1.9924\n",
      "Epoch: 083, Loss: 2.0966\n",
      "Epoch: 084, Loss: 1.8843\n",
      "Epoch: 085, Loss: 1.8458\n",
      "Epoch: 086, Loss: 1.8859\n",
      "Epoch: 087, Loss: 1.8395\n",
      "Epoch: 088, Loss: 1.8242\n",
      "Epoch: 089, Loss: 1.9011\n",
      "Epoch: 090, Loss: 1.8544\n",
      "Epoch: 091, Loss: 1.8437\n",
      "Epoch: 092, Loss: 1.8311\n",
      "Epoch: 093, Loss: 1.8681\n",
      "Epoch: 094, Loss: 1.7872\n",
      "Epoch: 095, Loss: 1.8397\n",
      "Epoch: 096, Loss: 1.8528\n",
      "Epoch: 097, Loss: 1.8490\n",
      "Epoch: 098, Loss: 1.9056\n",
      "Epoch: 099, Loss: 1.8004\n",
      "Epoch: 100, Loss: 1.8567\n",
      "Epoch: 101, Loss: 1.8136\n",
      "Epoch: 102, Loss: 1.7199\n",
      "Epoch: 103, Loss: 1.8106\n",
      "Epoch: 104, Loss: 1.7571\n",
      "Epoch: 105, Loss: 1.8605\n",
      "Epoch: 106, Loss: 1.7110\n",
      "Epoch: 107, Loss: 1.7944\n",
      "Epoch: 108, Loss: 1.6965\n",
      "Epoch: 109, Loss: 1.8016\n",
      "Epoch: 110, Loss: 1.7503\n",
      "Epoch: 111, Loss: 1.7253\n",
      "Epoch: 112, Loss: 1.7127\n",
      "Epoch: 113, Loss: 1.8200\n",
      "Epoch: 114, Loss: 1.7548\n",
      "Epoch: 115, Loss: 1.6851\n",
      "Epoch: 116, Loss: 1.7058\n",
      "Epoch: 117, Loss: 1.7248\n",
      "Epoch: 118, Loss: 1.6888\n",
      "Epoch: 119, Loss: 1.7067\n",
      "Epoch: 120, Loss: 1.6961\n",
      "Epoch: 121, Loss: 1.6699\n",
      "Epoch: 122, Loss: 1.6663\n",
      "Epoch: 123, Loss: 1.7450\n",
      "Epoch: 124, Loss: 1.7242\n",
      "Epoch: 125, Loss: 1.6710\n",
      "Epoch: 126, Loss: 1.6636\n",
      "Epoch: 127, Loss: 1.6565\n",
      "Epoch: 128, Loss: 1.6578\n",
      "Epoch: 129, Loss: 1.6804\n",
      "Epoch: 130, Loss: 1.6663\n",
      "Epoch: 131, Loss: 1.6609\n",
      "Epoch: 132, Loss: 1.6438\n",
      "Epoch: 133, Loss: 1.6580\n",
      "Epoch: 134, Loss: 1.6492\n",
      "Epoch: 135, Loss: 1.6428\n",
      "Epoch: 136, Loss: 1.6610\n",
      "Epoch: 137, Loss: 1.6657\n",
      "Epoch: 138, Loss: 1.6229\n",
      "Epoch: 139, Loss: 1.7255\n",
      "Epoch: 140, Loss: 1.6496\n",
      "Epoch: 141, Loss: 1.6301\n",
      "Epoch: 142, Loss: 1.6722\n",
      "Epoch: 143, Loss: 1.7497\n",
      "Epoch: 144, Loss: 1.7175\n",
      "Epoch: 145, Loss: 1.6826\n",
      "Epoch: 146, Loss: 1.6460\n",
      "Epoch: 147, Loss: 1.6320\n",
      "Epoch: 148, Loss: 1.6950\n",
      "Epoch: 149, Loss: 1.6412\n",
      "Epoch: 150, Loss: 1.6459\n",
      "Epoch: 151, Loss: 1.6192\n",
      "Epoch: 152, Loss: 1.6286\n",
      "Epoch: 153, Loss: 1.6318\n",
      "Epoch: 154, Loss: 1.6206\n",
      "Epoch: 155, Loss: 1.6286\n",
      "Epoch: 156, Loss: 1.6067\n",
      "Epoch: 157, Loss: 1.6034\n",
      "Epoch: 158, Loss: 1.6393\n",
      "Epoch: 159, Loss: 1.6360\n",
      "Epoch: 160, Loss: 1.6275\n",
      "Epoch: 161, Loss: 1.6094\n",
      "Epoch: 162, Loss: 1.6038\n",
      "Epoch: 163, Loss: 1.6514\n",
      "Epoch: 164, Loss: 1.6048\n",
      "Epoch: 165, Loss: 1.6219\n",
      "Epoch: 166, Loss: 1.6061\n",
      "Epoch: 167, Loss: 1.6145\n",
      "Epoch: 168, Loss: 1.5887\n",
      "Epoch: 169, Loss: 1.6096\n",
      "Epoch: 170, Loss: 1.5871\n",
      "Epoch: 171, Loss: 1.5877\n",
      "Epoch: 172, Loss: 1.5967\n",
      "Epoch: 173, Loss: 2.2409\n",
      "Epoch: 174, Loss: 1.5949\n",
      "Epoch: 175, Loss: 1.6066\n",
      "Epoch: 176, Loss: 1.6010\n",
      "Epoch: 177, Loss: 1.6359\n",
      "Epoch: 178, Loss: 1.5943\n",
      "Epoch: 179, Loss: 1.5854\n",
      "Epoch: 180, Loss: 1.5924\n",
      "Epoch: 181, Loss: 1.6366\n",
      "Epoch: 182, Loss: 1.6073\n",
      "Epoch: 183, Loss: 1.5894\n",
      "Epoch: 184, Loss: 1.5947\n",
      "Epoch: 185, Loss: 1.5888\n",
      "Epoch: 186, Loss: 1.5852\n",
      "Epoch: 187, Loss: 1.5710\n",
      "Epoch: 188, Loss: 1.5729\n",
      "Epoch: 189, Loss: 1.5945\n",
      "Epoch: 190, Loss: 1.5910\n",
      "Epoch: 191, Loss: 1.5940\n",
      "Epoch: 192, Loss: 1.6193\n",
      "Epoch: 193, Loss: 1.5634\n",
      "Epoch: 194, Loss: 1.5666\n",
      "Epoch: 195, Loss: 1.5553\n",
      "Epoch: 196, Loss: 1.5663\n",
      "Epoch: 197, Loss: 1.5802\n",
      "Epoch: 198, Loss: 1.5614\n",
      "Epoch: 199, Loss: 1.5773\n",
      "Epoch: 200, Loss: 1.5702\n",
      "GAT(\n",
      "  (layers): ModuleList(\n",
      "    (0): GATConv(500, 8, heads=8)\n",
      "    (1): GATConv(64, 8, heads=8)\n",
      "    (2): GATConv(64, 8, heads=8)\n",
      "    (3): GATConv(64, 7, heads=1)\n",
      "  )\n",
      ")\n",
      "Epoch: 001, Loss: 23.6290\n",
      "Epoch: 002, Loss: 18.3593\n",
      "Epoch: 003, Loss: 15.5252\n",
      "Epoch: 004, Loss: 13.4780\n",
      "Epoch: 005, Loss: 10.2531\n",
      "Epoch: 006, Loss: 10.7424\n",
      "Epoch: 007, Loss: 7.4932\n",
      "Epoch: 008, Loss: 6.7934\n",
      "Epoch: 009, Loss: 5.4966\n",
      "Epoch: 010, Loss: 5.6766\n",
      "Epoch: 011, Loss: 5.6353\n",
      "Epoch: 012, Loss: 5.1209\n",
      "Epoch: 013, Loss: 4.7703\n",
      "Epoch: 014, Loss: 4.9789\n",
      "Epoch: 015, Loss: 4.6826\n",
      "Epoch: 016, Loss: 4.3064\n",
      "Epoch: 017, Loss: 4.0920\n",
      "Epoch: 018, Loss: 4.0257\n",
      "Epoch: 019, Loss: 4.1683\n",
      "Epoch: 020, Loss: 3.7435\n",
      "Epoch: 021, Loss: 3.6140\n",
      "Epoch: 022, Loss: 3.3182\n",
      "Epoch: 023, Loss: 3.5568\n",
      "Epoch: 024, Loss: 3.2686\n",
      "Epoch: 025, Loss: 3.3590\n",
      "Epoch: 026, Loss: 3.3319\n",
      "Epoch: 027, Loss: 3.3489\n",
      "Epoch: 028, Loss: 3.2011\n",
      "Epoch: 029, Loss: 3.3494\n",
      "Epoch: 030, Loss: 3.0665\n",
      "Epoch: 031, Loss: 3.0808\n",
      "Epoch: 032, Loss: 2.8945\n",
      "Epoch: 033, Loss: 3.1009\n",
      "Epoch: 034, Loss: 2.8234\n",
      "Epoch: 035, Loss: 2.7425\n",
      "Epoch: 036, Loss: 2.7548\n",
      "Epoch: 037, Loss: 2.8616\n",
      "Epoch: 038, Loss: 2.6505\n",
      "Epoch: 039, Loss: 2.7983\n",
      "Epoch: 040, Loss: 2.5774\n",
      "Epoch: 041, Loss: 2.7633\n",
      "Epoch: 042, Loss: 2.7529\n",
      "Epoch: 043, Loss: 2.6062\n",
      "Epoch: 044, Loss: 2.5636\n",
      "Epoch: 045, Loss: 2.5278\n",
      "Epoch: 046, Loss: 2.6344\n",
      "Epoch: 047, Loss: 2.4469\n",
      "Epoch: 048, Loss: 2.5269\n",
      "Epoch: 049, Loss: 2.4888\n",
      "Epoch: 050, Loss: 2.4711\n",
      "Epoch: 051, Loss: 2.4625\n",
      "Epoch: 052, Loss: 2.4096\n",
      "Epoch: 053, Loss: 2.3642\n",
      "Epoch: 054, Loss: 2.2761\n",
      "Epoch: 055, Loss: 2.3485\n",
      "Epoch: 056, Loss: 2.4814\n",
      "Epoch: 057, Loss: 2.2778\n",
      "Epoch: 058, Loss: 2.3313\n",
      "Epoch: 059, Loss: 2.2867\n",
      "Epoch: 060, Loss: 2.1764\n",
      "Epoch: 061, Loss: 2.3113\n",
      "Epoch: 062, Loss: 2.1862\n",
      "Epoch: 063, Loss: 2.3122\n",
      "Epoch: 064, Loss: 2.2410\n",
      "Epoch: 065, Loss: 2.2610\n",
      "Epoch: 066, Loss: 2.3396\n",
      "Epoch: 067, Loss: 2.3391\n",
      "Epoch: 068, Loss: 2.3057\n",
      "Epoch: 069, Loss: 2.1236\n",
      "Epoch: 070, Loss: 2.1272\n",
      "Epoch: 071, Loss: 2.1020\n",
      "Epoch: 072, Loss: 1.9836\n",
      "Epoch: 073, Loss: 2.0853\n",
      "Epoch: 074, Loss: 2.1505\n",
      "Epoch: 075, Loss: 2.1142\n",
      "Epoch: 076, Loss: 2.0441\n",
      "Epoch: 077, Loss: 2.1308\n",
      "Epoch: 078, Loss: 2.0431\n",
      "Epoch: 079, Loss: 1.9793\n",
      "Epoch: 080, Loss: 2.0709\n",
      "Epoch: 081, Loss: 1.9987\n",
      "Epoch: 082, Loss: 1.9890\n",
      "Epoch: 083, Loss: 2.0500\n",
      "Epoch: 084, Loss: 1.9979\n",
      "Epoch: 085, Loss: 1.9814\n",
      "Epoch: 086, Loss: 2.0408\n",
      "Epoch: 087, Loss: 1.9888\n",
      "Epoch: 088, Loss: 1.9280\n",
      "Epoch: 089, Loss: 1.9316\n",
      "Epoch: 090, Loss: 1.9060\n",
      "Epoch: 091, Loss: 1.9751\n",
      "Epoch: 092, Loss: 1.9088\n",
      "Epoch: 093, Loss: 1.9157\n",
      "Epoch: 094, Loss: 1.9451\n",
      "Epoch: 095, Loss: 1.9334\n",
      "Epoch: 096, Loss: 1.8742\n",
      "Epoch: 097, Loss: 1.8696\n",
      "Epoch: 098, Loss: 1.8354\n",
      "Epoch: 099, Loss: 1.9511\n",
      "Epoch: 100, Loss: 1.8457\n",
      "Epoch: 101, Loss: 1.8665\n",
      "Epoch: 102, Loss: 1.8605\n",
      "Epoch: 103, Loss: 1.8769\n",
      "Epoch: 104, Loss: 1.8465\n",
      "Epoch: 105, Loss: 1.8437\n",
      "Epoch: 106, Loss: 1.7497\n",
      "Epoch: 107, Loss: 1.8497\n",
      "Epoch: 108, Loss: 1.8085\n",
      "Epoch: 109, Loss: 1.7904\n",
      "Epoch: 110, Loss: 1.7804\n",
      "Epoch: 111, Loss: 1.8373\n",
      "Epoch: 112, Loss: 1.7368\n",
      "Epoch: 113, Loss: 1.7761\n",
      "Epoch: 114, Loss: 1.8089\n",
      "Epoch: 115, Loss: 1.8504\n",
      "Epoch: 116, Loss: 1.7916\n",
      "Epoch: 117, Loss: 1.7921\n",
      "Epoch: 118, Loss: 1.7493\n",
      "Epoch: 119, Loss: 1.7872\n",
      "Epoch: 120, Loss: 1.7981\n",
      "Epoch: 121, Loss: 1.8332\n",
      "Epoch: 122, Loss: 1.7823\n",
      "Epoch: 123, Loss: 1.7827\n",
      "Epoch: 124, Loss: 1.7490\n",
      "Epoch: 125, Loss: 1.7793\n",
      "Epoch: 126, Loss: 1.8150\n",
      "Epoch: 127, Loss: 1.7207\n",
      "Epoch: 128, Loss: 1.7625\n",
      "Epoch: 129, Loss: 1.7273\n",
      "Epoch: 130, Loss: 1.7952\n",
      "Epoch: 131, Loss: 1.7735\n",
      "Epoch: 132, Loss: 1.7204\n",
      "Epoch: 133, Loss: 1.7553\n",
      "Epoch: 134, Loss: 1.7250\n",
      "Epoch: 135, Loss: 1.7222\n",
      "Epoch: 136, Loss: 1.7355\n",
      "Epoch: 137, Loss: 1.6593\n",
      "Epoch: 138, Loss: 1.8217\n",
      "Epoch: 139, Loss: 1.7077\n",
      "Epoch: 140, Loss: 1.6629\n",
      "Epoch: 141, Loss: 1.7202\n",
      "Epoch: 142, Loss: 1.6851\n",
      "Epoch: 143, Loss: 1.6582\n",
      "Epoch: 144, Loss: 1.6938\n",
      "Epoch: 145, Loss: 1.7005\n",
      "Epoch: 146, Loss: 1.6899\n",
      "Epoch: 147, Loss: 1.7351\n",
      "Epoch: 148, Loss: 1.6672\n",
      "Epoch: 149, Loss: 1.7796\n",
      "Epoch: 150, Loss: 1.6881\n",
      "Epoch: 151, Loss: 1.6450\n",
      "Epoch: 152, Loss: 1.7148\n",
      "Epoch: 153, Loss: 1.6593\n",
      "Epoch: 154, Loss: 1.7107\n",
      "Epoch: 155, Loss: 1.6899\n",
      "Epoch: 156, Loss: 1.6496\n",
      "Epoch: 157, Loss: 1.6380\n",
      "Epoch: 158, Loss: 1.6845\n",
      "Epoch: 159, Loss: 1.6299\n",
      "Epoch: 160, Loss: 1.6418\n",
      "Epoch: 161, Loss: 1.6590\n",
      "Epoch: 162, Loss: 1.6505\n",
      "Epoch: 163, Loss: 1.6484\n",
      "Epoch: 164, Loss: 1.6501\n",
      "Epoch: 165, Loss: 1.6776\n",
      "Epoch: 166, Loss: 1.6478\n",
      "Epoch: 167, Loss: 1.6439\n",
      "Epoch: 168, Loss: 1.6141\n",
      "Epoch: 169, Loss: 1.6212\n",
      "Epoch: 170, Loss: 1.6216\n",
      "Epoch: 171, Loss: 1.6298\n",
      "Epoch: 172, Loss: 1.6452\n",
      "Epoch: 173, Loss: 1.6100\n",
      "Epoch: 174, Loss: 1.6597\n",
      "Epoch: 175, Loss: 1.6282\n",
      "Epoch: 176, Loss: 1.7417\n",
      "Epoch: 177, Loss: 1.6128\n",
      "Epoch: 178, Loss: 1.5997\n",
      "Epoch: 179, Loss: 1.6166\n",
      "Epoch: 180, Loss: 1.6182\n",
      "Epoch: 181, Loss: 1.6339\n",
      "Epoch: 182, Loss: 1.5951\n",
      "Epoch: 183, Loss: 1.6421\n",
      "Epoch: 184, Loss: 1.6109\n",
      "Epoch: 185, Loss: 1.5890\n",
      "Epoch: 186, Loss: 1.6182\n",
      "Epoch: 187, Loss: 1.6059\n",
      "Epoch: 188, Loss: 1.5944\n",
      "Epoch: 189, Loss: 1.6115\n",
      "Epoch: 190, Loss: 1.5943\n",
      "Epoch: 191, Loss: 1.6644\n",
      "Epoch: 192, Loss: 1.6020\n",
      "Epoch: 193, Loss: 1.5927\n",
      "Epoch: 194, Loss: 1.5986\n",
      "Epoch: 195, Loss: 1.5975\n",
      "Epoch: 196, Loss: 1.6558\n",
      "Epoch: 197, Loss: 1.6100\n",
      "Epoch: 198, Loss: 1.5667\n",
      "Epoch: 199, Loss: 1.5858\n",
      "Epoch: 200, Loss: 1.5914\n"
     ]
    }
   ],
   "source": [
    "run_ids=[1,2]\n",
    "for i,run_id in enumerate(run_ids):\n",
    "    config = {\n",
    "        \"model_name\":\"GAT\",\n",
    "        \"task\":\"NC\",\n",
    "        \"run_id\":run_id,\n",
    "        \"dataset\":dataset_name\n",
    "    }\n",
    "    path = 'model_data/'+config['dataset']+\"/\"+config['model_name']+\"/\"\n",
    "    !mkdir -p $path\n",
    "    path2 = 'model_data/'+config['dataset']+\"/\"+config['model_name']+\"/\"+config['task']+\"_\"+str(config['run_id'])+\"*\"\n",
    "    !rm $path2\n",
    "    model = GAT(seeds[i],hidden_channels=8, out_channels=dataset.num_classes,num_layers=4,heads=8,dropout=0.2)\n",
    "    model, data = model.to(device), data.to(device)\n",
    "    print(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    loss_list = []\n",
    "    test_acc_list = []\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        loss_list.append(loss)\n",
    "        test_acc = test()\n",
    "        test_acc_list.append(test_acc)\n",
    "        feature_vals = deepcopy(model.feature_vals)\n",
    "        feature_path =  path+config['task']+\"_\"+str(config['run_id'])+'_'+str(epoch)+'.npz'\n",
    "        np.savez(feature_path, **feature_vals)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0cd37a9e-2204-403a-9741-dee5865b0a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4841\n"
     ]
    }
   ],
   "source": [
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "76b66210-896b-47a4-bf9e-3f62bf2d32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #from matplotlib.pyplot import plt\n",
    "# plt.figure(figsize=(20,8))\n",
    "# plt.plot(test_acc_list)\n",
    "# plt.title(\"Accuracy Over Epochs\", fontsize=25)\n",
    "# plt.xlabel(\"Epochs\", fontsize=25)\n",
    "# plt.ylabel(\"Test Accuracy\", fontsize=25)\n",
    "# output_filename = f'GAT_NC_test_accuracy.png'\n",
    "# # Save the heatmap plot as an image\n",
    "# plt.xticks(fontsize=25)\n",
    "# plt.yticks(fontsize=25)\n",
    "# plt.savefig(output_filename, bbox_inches='tight')\n",
    "# #plt.show()\n",
    "# plt.close()  # Close the plot to release resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d6791cdd-b3b8-4a1d-ac20-212f994b70bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89250, 64)\n",
      "(89250, 64)\n",
      "(89250, 64)\n",
      "(89250, 7)\n"
     ]
    }
   ],
   "source": [
    "for k in model.feature_vals.keys():\n",
    "    print(model.feature_vals[k].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc282b17-d9c1-425e-944e-ca3b6b1e28ba",
   "metadata": {},
   "source": [
    "## Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0f774331-cd22-4e0d-a00d-b6fd858fbb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Pubmed():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 500\n",
      "Number of classes: 3\n",
      "\n",
      "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])\n",
      "===========================================================================================================\n",
      "Number of nodes: 19717\n",
      "Number of edges: 88648\n",
      "Average node degree: 4.50\n",
      "Number of training nodes: 60\n",
      "Training node label rate: 0.00\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid, Flickr, Amazon\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "\n",
    "if dataset_name=='Flickr':\n",
    "    transform = Compose([\n",
    "        #NormalizeFeatures(),\n",
    "        RandomNodeSplit('train_rest',num_val = 2000, num_test = 10000)\n",
    "    ])\n",
    "    dataset = Flickr(root='data/Flickr', \\\n",
    "                     transform =transform)\n",
    "elif dataset_name=='Amazon':\n",
    "    transform = Compose([\n",
    "        #NormalizeFeatures(),\n",
    "        RandomNodeSplit('train_rest',num_val = 1000, num_test = 3000)\n",
    "    ])\n",
    "    dataset = Amazon(root='data/Amazon', name='Computers', \\\n",
    "                     transform =transform)\n",
    "\n",
    "elif dataset_name in ['Cora', 'Citeseer', 'Pubmed']:\n",
    "    # For Planetoid datasets, the standard split is already defined\n",
    "    dataset = Planetoid(root=f'data/{dataset_name}', name=dataset_name)\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('===========================================================================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "710cf45d-df5c-4824-8aed-2cb063bd78ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danish/anaconda3/envs/NSA_v1/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[19717, 500], val_pos_edge_index=[2, 2216], test_pos_edge_index=[2, 4432], train_pos_edge_index=[2, 75352], train_neg_adj_mask=[19717, 19717], val_neg_edge_index=[2, 2216], test_neg_edge_index=[2, 4432])\n"
     ]
    }
   ],
   "source": [
    "data.train_mask = data.val_mask = data.test_mask = data.y = None\n",
    "data = train_test_split_edges(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "084693ce-2975-4f6d-9554-255a334f4555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, seed, hidden_channels, out_channels, num_layers, heads=8, dropout=0.3):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.feature_vals = {}\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.dropout = dropout\n",
    "        self.layers.append(GATConv(dataset.num_features, hidden_channels, heads=heads))\n",
    "        for i in range(num_layers-2):\n",
    "            self.layers.append(GATConv(hidden_channels*heads, hidden_channels, heads=heads))\n",
    "        self.layers.append(GATConv(hidden_channels*heads, out_channels, heads=heads))\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            x = layer(x, edge_index)\n",
    "            if i!= (len(self.layers)-1):\n",
    "                x = F.elu(x, alpha=1)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            self.feature_vals['conv'+str(i)] = deepcopy(x.detach().cpu().numpy())\n",
    "        return x\n",
    "\n",
    "\n",
    "    def decode(self, z, pos_edge_index, neg_edge_index): # only pos and neg edges\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1) # concatenate pos and neg edges\n",
    "        logits = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)  # dot product \n",
    "        return logits\n",
    "\n",
    "    def decode_all(self, z): \n",
    "        prob_adj = z @ z.t() # get adj NxN\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t() # get predicted edge_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b61b23-61f0-4de6-b60e-be555df00b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d95e4cad-122b-4a57-9dd2-b9efbe4a5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    # returns a tensor:\n",
    "    # [1,1,1,1,...,0,0,0,0,0,..] with the number of ones is equel to the length of pos_edge_index\n",
    "    # and the number of zeros is equal to the length of neg_edge_index\n",
    "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(E, dtype=torch.float, device=device)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1.\n",
    "    return link_labels\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.train_pos_edge_index, #positive edges\n",
    "        num_nodes=data.num_nodes, # number of nodes\n",
    "        num_neg_samples=data.train_pos_edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    z = model.encode(data.x, data.train_pos_edge_index) #encode\n",
    "    link_logits = model.decode(z, data.train_pos_edge_index, neg_edge_index) # decode\n",
    "    \n",
    "    link_labels = get_link_labels(data.train_pos_edge_index, neg_edge_index)\n",
    "    loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    perfs = []\n",
    "    for prefix in [\"val\", \"test\"]:\n",
    "        pos_edge_index = data[f'{prefix}_pos_edge_index']\n",
    "        neg_edge_index = data[f'{prefix}_neg_edge_index']\n",
    "\n",
    "        z = model.encode(data.x, data.train_pos_edge_index) # encode train\n",
    "        link_logits = model.decode(z, pos_edge_index, neg_edge_index) # decode test or val\n",
    "        link_probs = link_logits.sigmoid() # apply sigmoid\n",
    "        \n",
    "        link_labels = get_link_labels(pos_edge_index, neg_edge_index) # get link\n",
    "        \n",
    "        perfs.append(roc_auc_score(link_labels.cpu(), link_probs.cpu())) #compute roc_auc score\n",
    "    return perfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b6aae1ed-1c1d-4e1b-81ae-d31b72e2cb4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘model_data/Pubmed/GAT/LP_1*’: No such file or directory\n",
      "Epoch: 010, Loss: 0.6909, Val: 0.7337, Test: 0.7395\n",
      "Epoch: 020, Loss: 0.6892, Val: 0.7337, Test: 0.7395\n",
      "Epoch: 030, Loss: 0.6866, Val: 0.7337, Test: 0.7395\n",
      "Epoch: 040, Loss: 0.6828, Val: 0.7337, Test: 0.7395\n",
      "Epoch: 050, Loss: 0.6771, Val: 0.7337, Test: 0.7395\n",
      "Epoch: 060, Loss: 0.6686, Val: 0.7337, Test: 0.7395\n",
      "Epoch: 070, Loss: 0.6559, Val: 0.7479, Test: 0.7501\n",
      "Epoch: 080, Loss: 0.6370, Val: 0.7684, Test: 0.7792\n",
      "Epoch: 090, Loss: 0.6148, Val: 0.7684, Test: 0.7792\n",
      "Epoch: 100, Loss: 0.5941, Val: 0.7684, Test: 0.7792\n",
      "Epoch: 110, Loss: 0.5846, Val: 0.7684, Test: 0.7792\n",
      "Epoch: 120, Loss: 0.5841, Val: 0.7684, Test: 0.7792\n",
      "Epoch: 130, Loss: 0.5811, Val: 0.7684, Test: 0.7792\n",
      "Epoch: 140, Loss: 0.5815, Val: 0.7684, Test: 0.7792\n",
      "Epoch: 150, Loss: 0.5790, Val: 0.7684, Test: 0.7792\n",
      "Epoch: 160, Loss: 0.5779, Val: 0.7684, Test: 0.7792\n",
      "Epoch: 170, Loss: 0.5767, Val: 0.7684, Test: 0.7762\n",
      "Epoch: 180, Loss: 0.5770, Val: 0.7700, Test: 0.7777\n",
      "Epoch: 190, Loss: 0.5767, Val: 0.7714, Test: 0.7789\n",
      "Epoch: 200, Loss: 0.5770, Val: 0.7727, Test: 0.7803\n",
      "rm: cannot remove ‘model_data/Pubmed/GAT/LP_2*’: No such file or directory\n",
      "Epoch: 010, Loss: 0.6910, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 020, Loss: 0.6895, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 030, Loss: 0.6871, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 040, Loss: 0.6835, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 050, Loss: 0.6787, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 060, Loss: 0.6713, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 070, Loss: 0.6607, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 080, Loss: 0.6459, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 090, Loss: 0.6249, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 100, Loss: 0.6034, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 110, Loss: 0.5865, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 120, Loss: 0.5837, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 130, Loss: 0.5819, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 140, Loss: 0.5791, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 150, Loss: 0.5772, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 160, Loss: 0.5750, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 170, Loss: 0.5749, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 180, Loss: 0.5716, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 190, Loss: 0.5701, Val: 0.8403, Test: 0.8423\n",
      "Epoch: 200, Loss: 0.5658, Val: 0.8403, Test: 0.8423\n"
     ]
    }
   ],
   "source": [
    "run_ids=[1,2]\n",
    "for i,run_id in enumerate(run_ids):\n",
    "    config = {\n",
    "        \"model_name\":\"GAT\",\n",
    "        \"task\":\"LP\",\n",
    "        \"run_id\":run_id,\n",
    "        \"dataset\":dataset_name\n",
    "    }\n",
    "    path = 'model_data/'+config['dataset']+\"/\"+config['model_name']+\"/\"\n",
    "    !mkdir -p $path\n",
    "    path2 = 'model_data/'+config['dataset']+\"/\"+config['model_name']+\"/\"+config['task']+\"_\"+str(config['run_id'])+\"*\"\n",
    "    model, data = Net(seeds[i],hidden_channels=8,out_channels=8,num_layers=4, heads=8, dropout=0).to(device), data.to(device)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-4)\n",
    "    test_acc_list=[]\n",
    "    !rm $path2\n",
    "    best_val_perf = test_perf = 0\n",
    "    for epoch in range(1, 201):\n",
    "        train_loss = train()\n",
    "        val_perf, tmp_test_perf = test()\n",
    "        test_acc_list.append(tmp_test_perf)\n",
    "        if val_perf > best_val_perf:\n",
    "            best_val_perf = val_perf\n",
    "            test_perf = tmp_test_perf\n",
    "        log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "        feature_vals = deepcopy(model.feature_vals)\n",
    "        feature_path =  path+config['task']+\"_\"+str(config['run_id'])+'_'+str(epoch)+'.npz'\n",
    "        np.savez(feature_path, **feature_vals)\n",
    "        if epoch % 10 == 0:\n",
    "            print(log.format(epoch, train_loss, best_val_perf, test_perf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2a0a0aee-5b24-4905-8f41-a309cc3ff216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = model.encode(data.x, data.train_pos_edge_index)\n",
    "# final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d58991f4-95bb-4aca-baa5-14a04a558085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19717, 64)\n",
      "(19717, 64)\n",
      "(19717, 64)\n",
      "(19717, 64)\n"
     ]
    }
   ],
   "source": [
    "for k in model.feature_vals.keys():\n",
    "    print(model.feature_vals[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "53890def-8ddd-47b7-9f92-d316d7a044bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #from matplotlib.pyplot import plt\n",
    "# plt.figure(figsize=(20,8))\n",
    "# plt.plot(test_acc_list)\n",
    "# plt.title(\"Accuracy Over Epochs\", fontsize=25)\n",
    "# plt.xlabel(\"Epochs\", fontsize=25)\n",
    "# plt.ylabel(\"Test Accuracy\", fontsize=25)\n",
    "# output_filename = f'GAT_LP_test_accuracy.png'\n",
    "# # Save the heatmap plot as an image\n",
    "# plt.xticks(fontsize=25)\n",
    "# plt.yticks(fontsize=25)\n",
    "# plt.savefig(output_filename, bbox_inches='tight')\n",
    "# #plt.show()\n",
    "# plt.close()  # Close the plot to release resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f850f2-5642-4fbd-a3b5-13c637eb6f22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
