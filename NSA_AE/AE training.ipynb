{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3190377",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from src.autoencoder import AutoEncoder, NSAAutoEncoder\n",
    "from src.utils import *\n",
    "from src.loss import RTDLoss, NSALoss, LID_NSALoss\n",
    "from src.top_ae import TopologicallyRegularizedAutoencoder\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bb080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"dataset_name\":\"F-MNIST\",\n",
    "    \"version\":\"k_L256_N256\",\n",
    "    \"model_name\":\"default\",\n",
    "    \"max_epochs\":250,\n",
    "    \"gpus\":[0],\n",
    "    \"rtd_every_n_batches\":1,\n",
    "    \"rtd_start_epoch\":60,\n",
    "    \"rtd_l\":1.0, # rtd loss\n",
    "    \"nsa_every_n_batches\":1,\n",
    "    \"nsa_start_epoch\":0,\n",
    "    \"nsa_l\":1.0, # rtd loss\n",
    "    \"n_runs\":1, # number of runs for each model\n",
    "    \"card\":50, # number of points on the persistence diagram\n",
    "    \"n_threads\":1, # number of threads for parallel ripser computation of pers homology\n",
    "    \"latent_dim\":16, # latent dimension (2 or 3 for vizualization purposes)\n",
    "    \"input_dim\":28*28,\n",
    "    \"n_hidden_layers\":3,\n",
    "    \"hidden_dim\":512,\n",
    "    \"batch_size\":256,\n",
    "    \"engine\":\"ripser\",\n",
    "    \"is_sym\":True,\n",
    "    \"lr\":1e-4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90495af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_dim, latent_dim=2, n_hidden_layers=2, m_type='encoder', **kwargs):\n",
    "    n = int(np.log2(input_dim))-1\n",
    "    layers = []\n",
    "    if m_type == 'encoder':\n",
    "        in_dim = input_dim\n",
    "        if input_dim  // 2 >= latent_dim:\n",
    "            out_dim = input_dim // 2\n",
    "        else:\n",
    "            out_dim = input_dim\n",
    "        for i in range(min(n, n_hidden_layers)):\n",
    "            layers.extend([nn.Linear(in_dim, out_dim), nn.ReLU()])\n",
    "            in_dim = out_dim\n",
    "            if in_dim  // 2 >= latent_dim:\n",
    "                out_dim = in_dim // 2\n",
    "            else:\n",
    "                out_dim = in_dim\n",
    "        layers.extend([nn.Linear(in_dim, latent_dim)])\n",
    "    elif m_type == 'decoder':\n",
    "        in_dim = latent_dim\n",
    "        out_dim = latent_dim * 2\n",
    "        for i in range(min(n, n_hidden_layers)):\n",
    "            layers.extend([nn.Linear(in_dim, out_dim), nn.ReLU()])\n",
    "            in_dim = out_dim\n",
    "            out_dim *= 2\n",
    "        layers.extend([nn.Linear(in_dim, input_dim)])\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def get_list_of_models(**config):\n",
    "    # define a list of models\n",
    "    encoder = get_linear_model(\n",
    "        m_type='encoder',\n",
    "        **config\n",
    "    )\n",
    "    decoder = get_linear_model(\n",
    "        m_type='decoder',\n",
    "        **config\n",
    "    )\n",
    "    models = {\n",
    "        # 'Basic AutoEncoder':AutoEncoder(\n",
    "        #    encoder = encoder,\n",
    "        #     decoder = decoder,\n",
    "        #     MSELoss = nn.MSELoss(),\n",
    "        #     **config\n",
    "        # ),\n",
    "        # 'Topological AutoEncoder':TopologicallyRegularizedAutoencoder(\n",
    "        #     encoder = encoder,\n",
    "        #     decoder = decoder,\n",
    "        #     MSELoss = nn.MSELoss(),\n",
    "        #     **config\n",
    "        # ),\n",
    "        # 'RTD AutoEncoder H1':AutoEncoder(\n",
    "        #     encoder = encoder,\n",
    "        #     decoder = decoder,\n",
    "        #     RTDLoss = RTDLoss(dim=1, lp=1.0,  **config), # only H1\n",
    "        #     MSELoss = nn.MSELoss(),\n",
    "        #     **config\n",
    "        # ),\n",
    "        'LID_NSA AutoEncoder':NSAAutoEncoder(\n",
    "            encoder = encoder,\n",
    "            decoder = decoder,\n",
    "            NSALoss = LID_NSALoss(k=config['batch_size']-1), # only H1\n",
    "            MSELoss = None,\n",
    "            **config\n",
    "        ),\n",
    "    }\n",
    "    return models, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81219803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_with_matrix(samples):\n",
    "    indicies, data, labels = zip(*samples)\n",
    "    data, labels = torch.tensor(np.asarray(data)), torch.tensor(np.asarray(labels))\n",
    "    if len(data.shape) > 2:\n",
    "        dist_data = torch.flatten(data, start_dim=1)\n",
    "    else:\n",
    "        dist_data = data\n",
    "    x_dist = torch.cdist(dist_data, dist_data, p=2) / np.sqrt(dist_data.shape[1])\n",
    "#     x_dist = (x_dist + x_dist.T) / 2.0 # make symmetrical (cdist is prone to computational errors)\n",
    "    return data, x_dist, labels\n",
    "\n",
    "def collate_with_matrix_geodesic(samples):\n",
    "    indicies, data, labels, dist_data = zip(*samples)\n",
    "    data, labels = torch.tensor(np.asarray(data)), torch.tensor(np.asarray(labels))\n",
    "    x_dist = torch.tensor(np.asarray(dist_data)[:, indicies])\n",
    "    return data, x_dist, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb1020",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = config['dataset_name']\n",
    "if dataset_name in ['COIL-20','COIL-100']:\n",
    "    train_data = np.load(f'data/{dataset_name}/prepared/data.npy').astype(np.float32)\n",
    "elif dataset_name.startswith('LinkPrediction'):\n",
    "    train_data = np.load(f'data/{dataset_name}/LP_3_200.npz')\n",
    "    train_data = dict(train_data)\n",
    "    print(train_data.keys())\n",
    "    key = list(train_data.keys())[-1]\n",
    "    print(key)\n",
    "    train_data = train_data[key]\n",
    "else:\n",
    "    train_data = np.load(f'data/{dataset_name}/prepared/train_data.npy').astype(np.float32)\n",
    "\n",
    "\n",
    "try:        \n",
    "    test_data = np.load(f'data/{dataset_name}/prepared/test_data.npy').astype(np.float32)\n",
    "except FileNotFoundError:\n",
    "    ids = np.random.choice(np.arange(len(train_data)), size=int(0.2*len(train_data)), replace=False)\n",
    "    test_data = train_data[ids]\n",
    "\n",
    "try:\n",
    "    if dataset_name in ['COIL-20','COIL-100']:\n",
    "        print(\"Inside here\")\n",
    "        train_labels = np.load(f'data/{dataset_name}/prepared/labels.npy')\n",
    "    elif dataset_name.startswith('LinkPrediction'):\n",
    "        train_labels = np.arange(1,len(train_data)+1)\n",
    "    else:\n",
    "        train_labels = np.load(f'data/{dataset_name}/prepared/train_labels.npy')\n",
    "except FileNotFoundError:\n",
    "    train_labels = None\n",
    "\n",
    "try:\n",
    "    test_labels = np.load(f'data/{dataset_name}/prepared/test_labels.npy')\n",
    "except FileNotFoundError:\n",
    "    if train_labels is None:\n",
    "        test_labels = None\n",
    "    else:\n",
    "        test_labels = train_labels[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3360bd60-ef8f-49b2-9343-440937ada962",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels[:10])\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels[:10])\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d89152-699b-4cf3-b04f-9a32e78702a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea3ca33-162b-4a76-acf4-9a5976f22a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Workflow\n",
    "\n",
    "#Now pick a random index and add that index to the queue\n",
    "#for each element in the queue, as you pop it\n",
    "#1 add all the neighbors in the neighbors to the queue, either k or k+ connections necessary to form a single connected component\n",
    "#Remove the node along with all its connections (edges) from the nearest neighbors graph and from potential indices that I can initially pick from\n",
    "#Repeat until I pop batch_size elements.\n",
    "#After one batch is ready, check for 1 cc and fix cc\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.sparse.csgraph import connected_components, shortest_path\n",
    "from src.utils import _fix_connected_components\n",
    "import copy\n",
    "\n",
    "\n",
    "class NearestNeighborBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size, num_neighbors):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_neighbors = num_neighbors\n",
    "        #Given input data X, compute k nearest neighbors for each point\n",
    "        self.kng = kneighbors_graph(self.dataset.data, n_neighbors=self.num_neighbors, mode='connectivity')\n",
    "        self.kng_dict_front = self.create_kng_dict_front()\n",
    "        self.kng_dict_back = self.create_kng_dict_back()\n",
    "        #Use fix connected components to generate one connected component\n",
    "        num_ccs, component_labels = connected_components(self.kng)\n",
    "        if num_ccs > 1:\n",
    "            self.kng = _fix_connected_components(self.dataset.data, self.kng, num_ccs, component_labels, mode='connectivity', metric='euclidean')\n",
    "            #Check again to confirm if it worked\n",
    "            num_ccs, component_labels = connected_components(self.kng)\n",
    "            if num_ccs >1:\n",
    "                raise ValueError(\"Increase nearest neighbor size; cannot generate a single connected component with the given knn size.\")\n",
    "\n",
    "    def create_kng_dict_front(self):\n",
    "        dict = [list(self.nearest_neighbors(i)) for i in range(len(self.dataset))]\n",
    "        return dict\n",
    "        \n",
    "    def create_kng_dict_back(self):\n",
    "        dict = [list(self.nearest_neighbors_of(i)) for i in range(len(self.dataset))]\n",
    "        return dict\n",
    "\n",
    "    def nearest_neighbors(self, index, kng=None):\n",
    "        if kng is None:\n",
    "            kng = self.kng\n",
    "        return kng.getrow(index).nonzero()[1]\n",
    "\n",
    "    def nearest_neighbors_of(self, index, kng=None):\n",
    "        if kng is None:\n",
    "            kng = self.kng\n",
    "        return kng.getcol(index).nonzero()[0]\n",
    "        \n",
    "    # def remove_point_existence(self, index):\n",
    "    #     for i in self.kng_dict_back[index]:\n",
    "    #         self.kng_dict_front[i].remove(index)\n",
    "    #     for i in self.kng_dict_front[index]:\n",
    "    #         self.kng_dict_back[i].remove(index)\n",
    "    #     self.kng_dict_front[index] = []\n",
    "    #     self.kng_dict_back[index] = []\n",
    "    \n",
    "    def __iter__(self):\n",
    "        print(\"Running iter\")\n",
    "        # iter_kng = self.kng.copy()\n",
    "        numbatches = len(self.dataset) // self.batch_size\n",
    "        indices = list(range(len(self.dataset)))\n",
    "        random.shuffle(indices)\n",
    "        indices = indices[:numbatches]\n",
    "        # self.kng_dict_front = self.create_kng_dict_front()\n",
    "        # self.kng_dict_back = self.create_kng_dict_back()\n",
    "        # indices = set(indices)\n",
    "        batches = []\n",
    "        while indices:\n",
    "            kng_front = copy.deepcopy(self.kng_dict_front)\n",
    "            kng_back = copy.deepcopy(self.kng_dict_back)\n",
    "            # if len(indices) < self.batch_size:\n",
    "            #     #Add functionality to sort the data in case you want to retain ordering\n",
    "            #     batches.append(indices)\n",
    "            #     break\n",
    "            # print(indices[:10])\n",
    "            batch = set()\n",
    "            queue = []\n",
    "            queue_set = set()\n",
    "            next_point = indices.pop()\n",
    "            # print(\"Current index:\",next_point)\n",
    "            # print(\"Remaining indices:\",len(indices))\n",
    "            # print(\"Number of batches:\", len(batches))\n",
    "            batch.add(next_point)\n",
    "            queue.extend(self.kng_dict_front[next_point])\n",
    "            queue_set.update(self.kng_dict_front[next_point])\n",
    "            for i in kng_back[next_point]:\n",
    "                kng_front[i].remove(next_point)\n",
    "            for i in kng_front[next_point]:\n",
    "                kng_back[i].remove(next_point)\n",
    "            kng_front[next_point] = []\n",
    "            kng_back[next_point] = []\n",
    "            # self.remove_point_existence(next_point)\n",
    "            while len(batch) + len(queue) < self.batch_size:\n",
    "                # print(\"Queue Length:\",len(queue))\n",
    "                if queue:\n",
    "                    next_point = queue.pop(0)\n",
    "                    if next_point not in batch:\n",
    "                        batch.add(next_point)\n",
    "                        # indices.remove(next_point)\n",
    "                        for new_point in self.kng_dict_front[next_point]:\n",
    "                            if new_point not in queue_set:\n",
    "                                queue.append(new_point)\n",
    "                                queue_set.add(new_point)\n",
    "                        for i in kng_back[next_point]:\n",
    "                            kng_front[i].remove(next_point)\n",
    "                        for i in kng_front[next_point]:\n",
    "                            kng_back[i].remove(next_point)\n",
    "                        kng_front[next_point] = []\n",
    "                        kng_back[next_point] = []\n",
    "                        # queue.extend(self.kng_dict_front[next_point])\n",
    "                        # self.remove_point_existence(next_point)\n",
    "                else:\n",
    "                #     # print(\"Queue is empty but batch requirements are not met starting from point:\",next_point)\n",
    "                    if indices:\n",
    "                        next_point = indices.pop()\n",
    "                        batch.add(next_point)\n",
    "                        queue.extend(self.kng_dict_front[next_point])\n",
    "                        queue_set.update(self.kng_dict_front[next_point])\n",
    "                        for i in kng_back[next_point]:\n",
    "                            kng_front[i].remove(next_point)\n",
    "                        for i in kng_front[next_point]:\n",
    "                            kng_back[i].remove(next_point)\n",
    "                        kng_front[next_point] = []\n",
    "                        kng_back[next_point] = []\n",
    "                        # queue.extend(self.kng_dict_front[next_point])\n",
    "                        # self.remove_point_existence(next_point)\n",
    "                # Remove duplicates in queue\n",
    "                #queue = list(dict.fromkeys(queue))\n",
    "            batch.update(queue)\n",
    "            # print(\"Created a batch of size:\",len(batch))\n",
    "        \n",
    "            batches.append(list(batch))\n",
    "            # print(\"Previous batch size:\", len(batches[-1]))\n",
    "            #This might not be necessary\n",
    "            # num_ccs, component_labels = connected_components(iter_kng)\n",
    "            # if num_ccs > 1:\n",
    "            #     print(\"Connected component check failed during batching\")\n",
    "            #     # iter_kng = _fix_connected_components(self.dataset.data, iter_kng, num_ccs, component_labels, mode='connectivity', metric='euclidean')\n",
    "            #     # #Check to see if it was fixed\n",
    "            #     # num_ccs, component_labels = connected_components(iter_kng)\n",
    "            #     # if n_ccs >1:\n",
    "            #     #     raise ValueError(\"Connected component check failed during batching; increase nearest neighbor size.\")\n",
    "        \n",
    "        return iter(batches)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b346e-8495-43ea-afb5-fd2807af91c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Workflow\n",
    "\n",
    "#Now pick a random index and add that index to the queue\n",
    "#for each element in the queue, as you pop it\n",
    "#1 add all the neighbors in the neighbors to the queue, either k or k+ connections necessary to form a single connected component\n",
    "#Remove the node along with all its connections (edges) from the nearest neighbors graph and from potential indices that I can initially pick from\n",
    "#Repeat until I pop batch_size elements.\n",
    "#After one batch is ready, check for 1 cc and fix cc\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.sparse.csgraph import connected_components, shortest_path\n",
    "from src.utils import _fix_connected_components\n",
    "import copy\n",
    "import threading, queue\n",
    "\n",
    "\n",
    "class NearestNeighborBatchSamplerMulti(Sampler):\n",
    "    def __init__(self, dataset, batch_size, num_neighbors, num_threads=24):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_neighbors = num_neighbors\n",
    "        #Given input data X, compute k nearest neighbors for each point\n",
    "        self.kng = kneighbors_graph(self.dataset.data, n_neighbors=self.num_neighbors, mode='connectivity')\n",
    "        self.kng_dict_front = self.create_kng_dict_front()\n",
    "        #Use fix connected components to generate one connected component\n",
    "        num_ccs, component_labels = connected_components(self.kng)\n",
    "        if num_ccs > 1:\n",
    "            self.kng = _fix_connected_components(self.dataset.data, self.kng, num_ccs, component_labels, mode='connectivity', metric='euclidean')\n",
    "            #Check again to confirm if it worked\n",
    "            num_ccs, component_labels = connected_components(self.kng)\n",
    "            if num_ccs >1:\n",
    "                raise ValueError(\"Increase nearest neighbor size; cannot generate a single connected component with the given knn size.\")\n",
    "        self.results_queue = queue.Queue()\n",
    "        self.num_threads = num_threads\n",
    "        self.indices = []\n",
    "\n",
    "    def create_kng_dict_front(self):\n",
    "        dict = [list(self.nearest_neighbors(i)) for i in range(len(self.dataset))]\n",
    "        return dict\n",
    "        \n",
    "    def create_kng_dict_back(self):\n",
    "        dict = [list(self.nearest_neighbors_of(i)) for i in range(len(self.dataset))]\n",
    "        return dict\n",
    "\n",
    "    def nearest_neighbors(self, index, kng=None):\n",
    "        if kng is None:\n",
    "            kng = self.kng\n",
    "        return kng.getrow(index).nonzero()[1]\n",
    "\n",
    "    def nearest_neighbors_of(self, index, kng=None):\n",
    "        if kng is None:\n",
    "            kng = self.kng\n",
    "        return kng.getcol(index).nonzero()[0]\n",
    "\n",
    "    def one_minibatch(self, point):\n",
    "        batch = set()\n",
    "        point_queue = []\n",
    "        queue_set = set()\n",
    "        batch.add(point)\n",
    "        point_queue.extend(self.kng_dict_front[point])\n",
    "        queue_set.update(self.kng_dict_front[point])\n",
    "        while len(batch) + len(point_queue) < self.batch_size:\n",
    "            if point_queue:\n",
    "                next_point = point_queue.pop(0)\n",
    "                if next_point not in batch:\n",
    "                    batch.add(next_point)\n",
    "                    for new_point in self.kng_dict_front[next_point]:\n",
    "                        if new_point not in queue_set:\n",
    "                            point_queue.append(new_point)\n",
    "                            queue_set.add(new_point)\n",
    "            else:\n",
    "                if self.indices:\n",
    "                    next_point = self.indices.pop()\n",
    "                    batch.add(next_point)\n",
    "                    point_queue.extend(self.kng_dict_front[next_point])\n",
    "                    queue_set.update(self.kng_dict_front[next_point])\n",
    "        batch.update(point_queue)\n",
    "        return batch\n",
    "        \n",
    "    def thread_minibatch(self):\n",
    "        while True:\n",
    "            try:\n",
    "                point = self.indices.pop()  # Get a value to process\n",
    "            except IndexError:\n",
    "                break\n",
    "            batch = self.one_minibatch(point)\n",
    "            self.results_queue.put(batch)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        #print(\"Running iter\")\n",
    "        numbatches = len(self.dataset) // self.batch_size\n",
    "        self.indices = list(range(len(self.dataset)))\n",
    "        random.shuffle(self.indices)\n",
    "        self.indices = self.indices[:numbatches]\n",
    "        batches = []\n",
    "        threads = []\n",
    "        for _ in range(self.num_threads):\n",
    "            thread = threading.Thread(target=self.thread_minibatch)\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "        while not self.results_queue.empty():\n",
    "            result = self.results_queue.get()\n",
    "            batches.append(list(result))\n",
    "        return iter(batches)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2dba75-dde6-4e70-9613-18131f7e6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels[:10])\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels[:10])\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d71a2-2fab-436c-9047-a9722771b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CustomMinMaxScaler:\n",
    "    def __init__(self):\n",
    "        self.min_vals = train_data.min()\n",
    "        self.max_vals = train_data.max()\n",
    "        self.is_fitted = True\n",
    "        \n",
    "    def fit(self, data):\n",
    "        self.min_vals = np.min(data, axis=0)\n",
    "        self.max_vals = np.max(data, axis=0)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "    def transform(self, data):\n",
    "        if not self.is_fitted:\n",
    "            raise NotFittedError\n",
    "        scaled_data = (data - self.min_vals) / (self.max_vals - self.min_vals)\n",
    "        return scaled_data\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d60562-287e-4175-9f25-7f77264b8cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a8e29-8571-4f29-b88e-2ba639946695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, k, **kwargs):\n",
    "        super().__init__(dataset, **kwargs)\n",
    "        self.k = k\n",
    "        self.epoch = 0\n",
    "    def __iter__(self):\n",
    "        if self.epoch % self.k == 0:\n",
    "            self._iterator = self._get_iterator()\n",
    "            self.epoch = 0  # Reset the epoch counter\n",
    "        self.epoch += 1\n",
    "        return self._iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc76f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = CustomMinMaxScaler()\n",
    "#scaler = None\n",
    "flatten = True\n",
    "geodesic = False\n",
    "\n",
    "train = FromNumpyDataset(\n",
    "    train_data, \n",
    "    train_labels, \n",
    "    geodesic=geodesic, \n",
    "    scaler=scaler, \n",
    "    flatten=flatten, \n",
    "    n_neighbors=2\n",
    ")\n",
    "print(\"Train done\")\n",
    "test = FromNumpyDataset(\n",
    "    test_data, \n",
    "    test_labels, \n",
    "    geodesic=geodesic, \n",
    "    scaler = train.scaler,    \n",
    "    flatten=flatten, \n",
    "    n_neighbors=2\n",
    ")\n",
    "train_sampler = NearestNeighborBatchSamplerMulti(train, config['batch_size'], num_neighbors=config['batch_size']-1, num_threads=24)\n",
    "#val_sampler = NearestNeighborBatchSampler(test, config['batch_size'], num_neighbors=5)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train, \n",
    "    batch_sampler=train_sampler, \n",
    "    num_workers=24, \n",
    "    collate_fn=collate_with_matrix_geodesic if geodesic else collate_with_matrix, \n",
    ")\n",
    "\n",
    "# train_loader = CustomDataLoader(\n",
    "#     train,\n",
    "#     batch_sampler=train_sampler,\n",
    "#     num_workers=0,\n",
    "#     k=10,\n",
    "#     collate_fn=collate_with_matrix_geodesic if geodesic else collate_with_matrix,\n",
    "# )\n",
    "\n",
    "# val_loader = DataLoader(\n",
    "#     test,\n",
    "#     batch_sampler=val_sampler,\n",
    "#     num_workers=2,\n",
    "#     collate_fn=collate_with_matrix_geodesic if geodesic else collate_with_matrix,\n",
    "# )\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     train, \n",
    "#     batch_size=config[\"batch_size\"], \n",
    "#     num_workers=24, \n",
    "#     collate_fn=collate_with_matrix_geodesic if geodesic else collate_with_matrix, \n",
    "#     shuffle=True,\n",
    "#     drop_last=True\n",
    "# )\n",
    "\n",
    "# val_loader = DataLoader(\n",
    "#     test,\n",
    "#     batch_size=config[\"batch_size\"],\n",
    "#     num_workers=24,\n",
    "#     collate_fn=collate_with_matrix_geodesic if geodesic else collate_with_matrix,\n",
    "#     shuffle=False,\n",
    "#     drop_last=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a12f432-1c1c-485b-9b64-600bd47519f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4398e80f-2e1a-43f8-a2a5-1a7b68e15187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a7a0ef-dab7-4985-8b10-12f616204adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1cdafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, train_loader, val_loader=None, model_name='default', \n",
    "                      dataset_name='F-MNIST', gpus=[0], max_epochs=100, run=0, version=\"d1\"):\n",
    "    version = f\"{dataset_name}_{model_name}_{version}_{run}\"\n",
    "    logger = pl.loggers.TensorBoardLogger(save_dir=os.getcwd(), name='lightning_logs', version=version)\n",
    "    trainer = pl.Trainer(\n",
    "        logger=logger, \n",
    "        gpus=gpus, \n",
    "        max_epochs=max_epochs, \n",
    "        log_every_n_steps=1, \n",
    "        num_sanity_val_steps=0\n",
    "    )\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    return model\n",
    "\n",
    "def dump_figures(figures, dataset_name, version):\n",
    "    for model_name in figures:\n",
    "        figures[model_name].savefig(f'results/{dataset_name}/{model_name}_{version}.png')\n",
    "\n",
    "def train_models(train_loader, val_loader, dataset_name=\"\", max_epochs=1, gpus=[], n_neighbors=[1], n_runs=1, version='', **kwargs):\n",
    "    models, encoder, decoder = get_list_of_models(**kwargs)\n",
    "    \n",
    "    for model_name in tqdm(models, desc=f\"Training models\"):\n",
    "        if 'AutoEncoder' in model_name: # train an autoencoder\n",
    "            models[model_name] = train_autoencoder(\n",
    "                models[model_name], \n",
    "                train_loader, \n",
    "                val_loader, \n",
    "                model_name, \n",
    "                dataset_name,\n",
    "                gpus,\n",
    "                max_epochs,\n",
    "                0,\n",
    "                version\n",
    "            )\n",
    "        else: # umap / pca / t-sne (sklearn interface)\n",
    "            train_latent = models[model_name].fit_transform(train_loader.dataset.data)\n",
    "        # measure training time\n",
    "    return encoder, decoder, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c06e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder, decoder, trained_models = train_models(train_loader, val_loader, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b2900-e1c7-4bf9-b5d6-118309296ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287502a3-cce4-4e14-8ea7-0ed1b72ea9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f891c5-ab96-4edc-97dc-ea159e91edf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b08b8-a917-4462-a964-2fc4710ccbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = config['version']\n",
    "train_loader = DataLoader(\n",
    "    train,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_with_matrix_geodesic if geodesic else collate_with_matrix,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "for model_name in trained_models:\n",
    "    latent, labels = get_latent_representations(trained_models[model_name], train_loader)\n",
    "    print(latent.shape)\n",
    "    np.save(f'data/{dataset_name}/{model_name}_latent_output_{version}.npy', latent)\n",
    "    np.save(f'data/{dataset_name}/{model_name}_latent_labels_{version}.npy', labels)\n",
    "\n",
    "for model_name in trained_models:\n",
    "    latent, labels = get_output_representations(trained_models[model_name], train_loader)\n",
    "    print(latent.shape)\n",
    "    np.save(f'data/{dataset_name}/{model_name}_final_output_{version}.npy', latent)\n",
    "    np.save(f'data/{dataset_name}/{model_name}_final_labels_{version}.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8a1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in trained_models:\n",
    "    latent, labels = get_latent_representations(trained_models[model_name], val_loader)\n",
    "    print(latent.shape)\n",
    "    np.save(f'data/{dataset_name}/{model_name}_latent_output_{version}_test.npy', latent)\n",
    "    np.save(f'data/{dataset_name}/{model_name}_latent_labels_{version}_test.npy', labels)\n",
    "\n",
    "for model_name in trained_models:\n",
    "    latent, labels = get_output_representations(trained_models[model_name], val_loader)\n",
    "    print(latent.shape)\n",
    "    np.save(f'data/{dataset_name}/{model_name}_final_output_{version}_test.npy', latent)\n",
    "    np.save(f'data/{dataset_name}/{model_name}_final_labels_{version}_test.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111eb76-3c90-4eb0-8855-b0af52b12d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfafc481-ca3e-4dc4-91a8-2669eae7f66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e894ecc6-20c0-4470-8efe-ca3fb0be5d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d3ce31-b511-4022-80a2-21944bdc1c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
