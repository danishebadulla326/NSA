{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cc4d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import gudhi as gd\n",
    "import gudhi.wasserstein as wasserstein\n",
    "import gudhi.hera as hera\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from itertools import combinations, combinations_with_replacement, product\n",
    "\n",
    "import ripserplusplus as rpp\n",
    "\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c6cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = 'viridis'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82d3dc",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d08f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'MNIST'\n",
    "version = 'd16'\n",
    "#version = 'd256_K32_N32_A_v1'\n",
    "\n",
    "models = {\n",
    "     'PCA':'PCA',\n",
    "     'UMAP':'UMAP',\n",
    "    'Basic AutoEncoder':'AE',\n",
    "    'Topological AutoEncoder':'TopoAE',\n",
    "    'RTD AutoEncoder H1':'RTD-AE',\n",
    "    'NSA AutoEncoder':'NSA-AE',\n",
    "    'LNSA AutoEncoder':'LNSA-AE',\n",
    "    'GNSA AutoEncoder':'GNSA-AE',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306043c0",
   "metadata": {},
   "source": [
    "## Calculate distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pdist_gpu(a, b, device = 'cuda'):\n",
    "    A = torch.tensor(a, dtype = torch.float64)\n",
    "    B = torch.tensor(b, dtype = torch.float64)\n",
    "\n",
    "    size = (A.shape[0] + B.shape[0]) * A.shape[1] / 1e9\n",
    "    max_size = 0.2\n",
    "\n",
    "    if size > max_size:\n",
    "        parts = int(size / max_size) + 1\n",
    "    else:\n",
    "        parts = 1\n",
    "\n",
    "    pdist = np.zeros((A.shape[0], B.shape[0]))\n",
    "    At = A.to(device)\n",
    "\n",
    "    for p in range(parts):\n",
    "        i1 = int(p * B.shape[0] / parts)\n",
    "        i2 = int((p + 1) * B.shape[0] / parts)\n",
    "        i2 = min(i2, B.shape[0])\n",
    "\n",
    "        Bt = B[i1:i2].to(device)\n",
    "        pt = torch.cdist(At, Bt)\n",
    "        pdist[:, i1:i2] = pt.cpu()\n",
    "\n",
    "        del Bt, pt\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    del At\n",
    "\n",
    "    return pdist\n",
    "\n",
    "def zero_out_diagonal(distances):# make 0 on diagonal\n",
    "    return distances * (np.ones_like(distances) - np.eye(*distances.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe527b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'COIL' in dataset_name:\n",
    "    data = np.load(f'data/{dataset_name}/prepared/data.npy')\n",
    "else:\n",
    "    data = np.load(f'data/{dataset_name}/prepared/train_data.npy')\n",
    "data = data.reshape(data.shape[0], -1)\n",
    "ids = np.random.choice(np.arange(len(data)), size=min(30000, len(data)), replace=False)\n",
    "data = data[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb13b31d-cba0-41bc-832e-33e05fb45627",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4dffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_distances = pdist_gpu(data, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be8c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_distances = zero_out_diagonal(original_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20754cfe",
   "metadata": {},
   "source": [
    "## Pearson correlation for pairwise distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if 'COIL' in dataset_name:\n",
    "        labels = np.load(f'data/{dataset_name}/prepared/labels.npy')\n",
    "    else:\n",
    "        labels = np.load(f'data/{dataset_name}/prepared/train_labels.npy')    \n",
    "except FileNotFoundError:\n",
    "    labels = np.load(f'data/{dataset_name}/prepared/train_data.npy')\n",
    "\n",
    "def get_distances(data):\n",
    "    data = data.reshape(data.shape[0], -1)\n",
    "    distances = distance_matrix(data, data)\n",
    "    distances = distances[np.triu(np.ones_like(distances), k=1) > 0]\n",
    "    return distances\n",
    " # take only different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf4d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for model_name in models:\n",
    "    try:\n",
    "        latent = np.load(f'data/{dataset_name}/{model_name}_latent_output_{version}.npy')[ids]\n",
    "        print(latent.shape)\n",
    "        latent_distances = pdist_gpu(latent, latent)\n",
    "    except FileNotFoundError:\n",
    "        latent = None\n",
    "        continue\n",
    "    results[model_name] = pearsonr(\n",
    "        latent_distances[np.triu(np.ones_like(original_distances), k=1) > 0], \n",
    "        original_distances[np.triu(np.ones_like(original_distances), k=1) > 0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e475b4-223b-40d1-a00f-3a62bda2170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8115d4e9",
   "metadata": {},
   "source": [
    "## Triplet accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_accuracy(input_data, latent_data, triplets=None):\n",
    "    # calculate distance matricies\n",
    "    input_data = input_data.reshape(input_data.shape[0], -1)\n",
    "    input_distances = zero_out_diagonal(pdist_gpu(input_data, input_data))\n",
    "    latent_data = latent_data.reshape(latent_data.shape[0], -1)\n",
    "    latent_distances = zero_out_diagonal(pdist_gpu(latent_data, latent_data))\n",
    "    # generate triplets\n",
    "    if triplets is None:\n",
    "        triplets = np.asarray(list(combinations(range(len(input_data)), r=3)))\n",
    "    i_s = triplets[:, 0]\n",
    "    j_s = triplets[:, 1]\n",
    "    k_s = triplets[:, 2]\n",
    "    acc = (np.logical_xor(\n",
    "        input_distances[i_s, j_s] < input_distances[i_s, k_s], \n",
    "        latent_distances[i_s, j_s] < latent_distances[i_s, k_s]\n",
    "    ) == False)\n",
    "    acc = np.mean(acc.astype(np.int32))\n",
    "    return acc\n",
    "\n",
    "\n",
    "def avg_triplet_accuracy(input_data, latent_data, batch_size=128, n_runs=20):\n",
    "    # average over batches\n",
    "    accs = []\n",
    "    triplets = np.asarray(list(combinations(range(min(batch_size, len(input_data))), r=3)))\n",
    "    if batch_size > len(input_data):\n",
    "        accs.append(triplet_accuracy(input_data, latent_data, triplets=triplets))\n",
    "        return accs\n",
    "    for _ in range(n_runs):\n",
    "        ids = np.random.choice(np.arange(len(input_data)), size=batch_size, replace=False)\n",
    "        accs.append(triplet_accuracy(input_data[ids], latent_data[ids], triplets=triplets))\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6716f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'COIL' in dataset_name:\n",
    "    input_data = np.load(f'data/{dataset_name}/prepared/data.npy')\n",
    "else:\n",
    "    input_data = np.load(f'data/{dataset_name}/prepared/train_data.npy')\n",
    "for model_name in models:\n",
    "    try:\n",
    "        latent_data = np.load(f'data/{dataset_name}/{model_name}_latent_output_{version}.npy')\n",
    "        print(latent_data.shape)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    accs = avg_triplet_accuracy(input_data, latent_data, batch_size=150, n_runs=10)\n",
    "    print(f\"Model: {model_name}, triplet acc: ${np.mean(accs):.3f} \\pm {np.std(accs):.3f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d68d1",
   "metadata": {},
   "source": [
    "# RTD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0dab6f",
   "metadata": {},
   "source": [
    "Switch to ripser++ from ArGentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa50da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loss import RTDLoss\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e3b39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "batch_size = 200\n",
    "\n",
    "loss = RTDLoss(dim=1, engine='ripser')\n",
    "if 'COIL' in dataset_name:\n",
    "    data = np.load(f'data/{dataset_name}/prepared/data.npy')\n",
    "else:\n",
    "    data = np.load(f'data/{dataset_name}/prepared/train_data.npy')\n",
    "\n",
    "# data = np.load(f'data/{dataset_name}/prepared/train_data.npy')\n",
    "data = data.reshape(len(data), -1)\n",
    "print(data.shape)\n",
    "if batch_size > len(data):\n",
    "    n_runs=1\n",
    "    \n",
    "max_dim = 1\n",
    "results = defaultdict(list)\n",
    "\n",
    "for i in tqdm(range(n_runs)):\n",
    "    ids = np.random.choice(np.arange(0, len(data)), size=min(batch_size, len(data)), replace=False)\n",
    "    \n",
    "    x = data[ids]\n",
    "    x_distances = distance_matrix(x, x)\n",
    "    x_distances = x_distances/np.percentile(x_distances.flatten(), 90)\n",
    "    \n",
    "    for model_name in models:\n",
    "        try:\n",
    "            z = np.load(f'data/{dataset_name}/{model_name}_latent_output_{version}.npy')\n",
    "        except FileNotFoundError:\n",
    "            try:\n",
    "                z = np.load(f'data/{dataset_name}/{model_name}_latent_output.npy')\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        z = z.reshape(len(z), -1)[ids]\n",
    "        z_distances = distance_matrix(z, z)\n",
    "        z_distances = z_distances/np.percentile(z_distances.flatten(), 90)\n",
    "        print(f'Calculating RTD for: {model_name}')\n",
    "        with torch.no_grad():\n",
    "            _, _, value = loss(torch.tensor(x_distances), torch.tensor(z_distances))\n",
    "        results[model_name].append(value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c728dbd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names = [\n",
    "    'PCA', \n",
    "    'UMAP', \n",
    "    'Basic AutoEncoder', \n",
    "    'Topological AutoEncoder', \n",
    "    'RTD AutoEncoder H1',\n",
    "    'NSA AutoEncoder',\n",
    "    'GNSA AutoEncoder',\n",
    "    'LNSA AutoEncoder',\n",
    "]\n",
    "for model_name in names:\n",
    "    if model_name in results:\n",
    "        print(f\"{model_name}: ${np.mean(results[model_name]):.2f} \\pm {np.std(results[model_name]):.2f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a032104",
   "metadata": {},
   "source": [
    "# Tripet acc. between cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_distances(data, labels):\n",
    "    clusters = []\n",
    "    if len(data.shape) > 2:\n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "    for l in np.sort(np.unique(labels)):\n",
    "        clusters.append(np.mean(data[labels == l], axis=0))\n",
    "    clusters = np.asarray(clusters)\n",
    "    return distance_matrix(clusters, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab3ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = np.load(f'data/{dataset_name}/prepared/train_data.npy')\n",
    "if 'COIL' in dataset_name:\n",
    "    data = np.load(f'data/{dataset_name}/prepared/data.npy')\n",
    "    labels = np.load(f'data/{dataset_name}/prepared/labels.npy')\n",
    "else:\n",
    "    data = np.load(f'data/{dataset_name}/prepared/train_data.npy')\n",
    "    labels = np.load(f'data/{dataset_name}/prepared/train_labels.npy')\n",
    "\n",
    "\n",
    "original_distances = get_cluster_distances(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc59f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_accuracy_between_clusters(original_distances, latent_distances):\n",
    "    triplets = np.asarray(list(combinations(range(len(original_distances)), r=3)))\n",
    "    i_s = triplets[:, 0]\n",
    "    j_s = triplets[:, 1]\n",
    "    k_s = triplets[:, 2]\n",
    "    acc = (np.logical_xor(\n",
    "        original_distances[i_s, j_s] < original_distances[i_s, k_s], \n",
    "        latent_distances[i_s, j_s] < latent_distances[i_s, k_s]\n",
    "    ) == False)\n",
    "    return acc\n",
    "\n",
    "def triplet_accuracy_between_clusters_(original_distances, latent_distances):\n",
    "    ids = range(len(original_distances))\n",
    "    triplets = np.asarray(list(product(ids, ids, ids)))\n",
    "    i_s = triplets[:, 0]\n",
    "    j_s = triplets[:, 1]\n",
    "    k_s = triplets[:, 2]\n",
    "    acc = (np.logical_xor(\n",
    "        original_distances[i_s, j_s] < original_distances[i_s, k_s], \n",
    "        latent_distances[i_s, j_s] < latent_distances[i_s, k_s]\n",
    "    ) == False)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8d3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    try:\n",
    "        latent_data = np.load(f'data/{dataset_name}/{model_name}_latent_output_{version}.npy')\n",
    "        print(latent_data.shape)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    latent_distances = get_cluster_distances(latent_data, labels)\n",
    "    accs = triplet_accuracy_between_clusters_(original_distances, latent_distances)\n",
    "    print(f\"Model: {model_name}, triplet acc: ${np.mean(accs):.3f} \\pm {np.std(accs):.3f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3d6ff1-8aaf-4c2c-8e5f-2380724e7542",
   "metadata": {},
   "source": [
    "### MSE Between X and X hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3f1f5-3e51-4c25-82ed-ca95f9f753da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60387a28-d26a-41bf-a085-880cb353a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name in ['COIL-20','COIL-100']:\n",
    "    data = np.load(f'data/{dataset_name}/prepared/data.npy')\n",
    "    data = data.reshape(len(data), -1)\n",
    "    labels = np.load(f'data/{dataset_name}/prepared/labels.npy')\n",
    "else:\n",
    "    data = np.load(f'data/{dataset_name}/prepared/train_data.npy')\n",
    "    data = data.reshape(len(data), -1)\n",
    "    labels = np.load(f'data/{dataset_name}/prepared/train_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d979de7-43cb-4f40-8401-e279f1c37bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = {\n",
    "    \"Basic AutoEncoder\":255,\n",
    "    \"RTD AutoEncoder H1\":255,\n",
    "    \"LNSA AutoEncoder\":255,\n",
    "    \"NSA AutoEncoder\":255,\n",
    "    \"GNSA AutoEncoder\":255,\n",
    "    \"Topological AutoEncoder\":255,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5a416-0966-42c6-9992-1e94b90d0d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    try:\n",
    "        output_data = np.load(f'data/{dataset_name}/{model_name}_final_output_{version}.npy')\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    accs = criterion(torch.tensor(data)/scaler[model_name], torch.tensor(output_data))\n",
    "    print(f\"Model: {model_name}, MSE value: {accs:e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e8ff8e-db18-4028-a5d0-3b9d6ce07656",
   "metadata": {},
   "source": [
    "#### Test MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da7243a-8ff6-41cd-b1ec-5dbb9c4ab702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No Test MSE for COIL Datasets\n",
    "\n",
    "data = np.load(f'data/{dataset_name}/prepared/test_data.npy')\n",
    "data = data.reshape(len(data), -1)\n",
    "labels = np.load(f'data/{dataset_name}/prepared/test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da1b24-717f-4edc-bc94-c1838f097082",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    try:\n",
    "        output_data = np.load(f'data/{dataset_name}/{model_name}_final_output_{version}_test.npy')\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    accs = criterion(torch.tensor(data)/scaler[model_name], torch.tensor(output_data))\n",
    "    print(f\"Model: {model_name}, MSE value: {accs:e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de2a6f-2d06-4422-949b-cfb56edfcf55",
   "metadata": {},
   "source": [
    "### Calculate GNSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c86f4e-d930-4070-bd11-fc24499dc6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loss import NSALoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fedf25-9313-43b9-8132-124a9eb05fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = NSALoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf9dd85-31c5-4ac6-9189-c302b5963e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "batch_size = 2000\n",
    "\n",
    "loss = criterion\n",
    "\n",
    "if dataset_name in ['COIL-20','COIL-100']:\n",
    "    data = np.load(f'data/{dataset_name}/prepared/data.npy')\n",
    "    data = data.reshape(len(data), -1)\n",
    "else:\n",
    "    data = np.load(f'data/{dataset_name}/prepared/train_data.npy')\n",
    "    data = data.reshape(len(data), -1)\n",
    "data = data/255\n",
    "\n",
    "if batch_size > len(data):\n",
    "    n_runs=1\n",
    "    \n",
    "results = defaultdict(list)\n",
    "\n",
    "for i in tqdm(range(n_runs)):\n",
    "    ids = np.random.choice(np.arange(0, len(data)), size=min(batch_size, len(data)), replace=False)\n",
    "    \n",
    "    x = data[ids]\n",
    "    x = x/255\n",
    "    \n",
    "    for model_name in models:\n",
    "        try:\n",
    "            z = np.load(f'data/{dataset_name}/{model_name}_latent_output_{version}.npy')\n",
    "        except FileNotFoundError:\n",
    "            try:\n",
    "                z = np.load(f'data/{dataset_name}/{model_name}_latent_output.npy')\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        z = z.reshape(len(z), -1)[ids]\n",
    "        print(f'Calculating NSA for: {model_name}')\n",
    "        with torch.no_grad():\n",
    "            value = loss(torch.tensor(x), torch.tensor(z))\n",
    "        results[model_name].append(value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b0a3f-968b-48a3-8d44-663df87bdcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    'PCA', \n",
    "    'UMAP', \n",
    "    'Basic AutoEncoder', \n",
    "    'Topological AutoEncoder', \n",
    "    'RTD AutoEncoder H1',\n",
    "    'NSA AutoEncoder',\n",
    "    'LNSA AutoEncoder',\n",
    "    'GNSA AutoEncoder',\n",
    "]\n",
    "for model_name in names:\n",
    "    if model_name in results:\n",
    "        print(f\"{model_name}: ${np.mean(results[model_name]):.4f} \\pm {np.std(results[model_name]):.2f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810e5508-c5a1-4474-afea-5a00c4892eed",
   "metadata": {},
   "source": [
    "### LNSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f05e42-f18a-4dc5-8ed4-59b5db410b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loss import LID_NSALoss_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f6515-5b72-4a2b-aba8-abefbdc1cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LID_NSALoss_v1(k=100,full=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce808eb-00e3-4959-befb-772d291381e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "batch_size = 4000\n",
    "\n",
    "loss = criterion\n",
    "\n",
    "if dataset_name in ['COIL-20','COIL-100']:\n",
    "    data = np.load(f'data/{dataset_name}/prepared/data.npy')\n",
    "    data = data.reshape(len(data), -1)\n",
    "else:\n",
    "    data = np.load(f'data/{dataset_name}/prepared/train_data.npy')\n",
    "    data = data.reshape(len(data), -1)\n",
    "data = data/255\n",
    "\n",
    "if batch_size > len(data):\n",
    "    n_runs=1\n",
    "    \n",
    "results = defaultdict(list)\n",
    "\n",
    "for i in tqdm(range(n_runs)):\n",
    "    ids = np.random.choice(np.arange(0, len(data)), size=min(batch_size, len(data)), replace=False)\n",
    "    \n",
    "    x = data[ids]\n",
    "    x = x/255\n",
    "    \n",
    "    for model_name in models:\n",
    "        try:\n",
    "            z = np.load(f'data/{dataset_name}/{model_name}_latent_output_{version}.npy')\n",
    "        except FileNotFoundError:\n",
    "            try:\n",
    "                z = np.load(f'data/{dataset_name}/{model_name}_latent_output.npy')\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        z = z.reshape(len(z), -1)[ids]\n",
    "        print(f'Calculating NSA for: {model_name}')\n",
    "        with torch.no_grad():\n",
    "            value1 = loss(torch.tensor(x), torch.tensor(z))\n",
    "            value2 = loss(torch.tensor(z), torch.tensor(x))\n",
    "        results[model_name].append((value1.item()+value2.item())/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25586e00-5570-47b8-886c-094eadeed3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    'PCA', \n",
    "    'UMAP', \n",
    "    'Basic AutoEncoder', \n",
    "    'Topological AutoEncoder', \n",
    "    'RTD AutoEncoder H1',\n",
    "    'NSA AutoEncoder',\n",
    "    'LNSA AutoEncoder',\n",
    "    'GNSA AutoEncoder',\n",
    "]\n",
    "for model_name in names:\n",
    "    if model_name in results:\n",
    "        print(f\"{model_name}: ${np.mean(results[model_name]):.4f} \\pm {np.std(results[model_name]):.2f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5221065a-8efc-4561-bddd-e516a2146d08",
   "metadata": {},
   "source": [
    "## K-NN Consistency\n",
    "\n",
    "How many of the k nearest neighbors are common between all the points in the two spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3619939f-8fe2-44c3-84e6-e91373a042cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import random\n",
    "\n",
    "def calculate_knn_consistency(space1, space2, k=5, num_points=10):\n",
    "    \"\"\"\n",
    "    Calculate the K-NN consistency between two spaces.\n",
    "\n",
    "    :param space1: numpy array of shape (n_samples, n_features_space1)\n",
    "    :param space2: numpy array of shape (n_samples, n_features_space2)\n",
    "    :param k: number of nearest neighbors to consider\n",
    "    :return: consistency score\n",
    "    \"\"\"\n",
    "    # Ensure the number of points is the same in both spaces\n",
    "    assert space1.shape[0] == space2.shape[0], \"The two spaces must have the same number of points\"\n",
    "\n",
    "    indices = random.sample(range(space1.shape[0]), num_points)\n",
    "\n",
    "    # Fit K-NN on both spaces\n",
    "    nn_space1 = NearestNeighbors(n_neighbors=k+1).fit(space1)\n",
    "    nn_space2 = NearestNeighbors(n_neighbors=k+1).fit(space2)\n",
    "\n",
    "    # Find K nearest neighbors in space1\n",
    "    consistency_count = 0\n",
    "    for i in indices:\n",
    "        # Find K nearest neighbors in both spaces for the selected points\n",
    "        neighbors_space1 = nn_space1.kneighbors([space1[i]], return_distance=False)[0]\n",
    "        neighbors_space2 = nn_space2.kneighbors([space2[i]], return_distance=False)[0]\n",
    "        # print(neighbors_space1)\n",
    "        # print(neighbors_space2)\n",
    "        # Exclude the point itself and calculate consistency\n",
    "        common_neighbors = set(neighbors_space1[1:]).intersection(neighbors_space2[1:])\n",
    "        consistency_count += len(common_neighbors)\n",
    "\n",
    "    # _, indices_space1 = nn_space1.kneighbors(space1)\n",
    "\n",
    "    # # Find K nearest neighbors in space2 for the corresponding points\n",
    "    # _, indices_space2 = nn_space2.kneighbors(space2)\n",
    "\n",
    "    # # Calculate consistency\n",
    "    # consistency_count = 0\n",
    "    # for i in range(space1.shape[0]):\n",
    "    #     # Use set intersection to find common neighbors; exclude the point itself (hence indices starting from 1)\n",
    "    #     common_neighbors = set(indices_space1[i, 1:]).intersection(indices_space2[i, 1:])\n",
    "    #     consistency_count += len(common_neighbors)\n",
    "\n",
    "    #consistency_score = consistency_count / (space1.shape[0] * k)\n",
    "    consistency_score = consistency_count / (num_points * k)\n",
    "    return consistency_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c491ca-4fcb-47b1-b303-6ff38fb4cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name in ['COIL-20','COIL-100']:\n",
    "    data = np.load(f'data/{dataset_name}/prepared/data.npy')\n",
    "    data = data.reshape(len(data), -1)\n",
    "else:\n",
    "    data = np.load(f'data/{dataset_name}/prepared/train_data.npy')\n",
    "    data = data.reshape(len(data), -1)\n",
    "data = data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f19cfb-c978-43be-9065-0d182dcb0dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    try:\n",
    "        latent_data = np.load(f'data/{dataset_name}/{model_name}_latent_output_{version}.npy')\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    print(data.shape)\n",
    "    print(latent_data.shape)\n",
    "    consistency_score = calculate_knn_consistency(data, latent_data, k=100, num_points=500)\n",
    "    print(f\"Model: {model_name}, k-NN consistency: {consistency_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d20550-0ae9-4ddb-a616-b1ecd6636261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a232ffc-2987-4259-bc94-474410c01cc8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Local Continuity Meta-Criterion (Not Used)\n",
    " \n",
    "The Local Continuity Meta-Criterion (LCMC) is a metric used to evaluate the quality of dimensionality reduction techniques, particularly in terms of how well local neighborhoods are preserved in the reduced dimensionality space. To compute the LCMC, you typically compare the ranks of distances in the original high-dimensional space with the ranks in the reduced low-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f4a18a-44bc-4aad-87e4-ae42563bebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def calculate_lcmc(original_space, reduced_space, k, num_points=2000):\n",
    "    \"\"\"\n",
    "    Calculate the Local Continuity Meta-Criterion (LCMC).\n",
    "\n",
    "    :param original_space: numpy array of shape (n_samples, n_features_original)\n",
    "    :param reduced_space: numpy array of shape (n_samples, n_features_reduced)\n",
    "    :param k: number of nearest neighbors to consider\n",
    "    :return: lcmc score\n",
    "    \"\"\"\n",
    "    indices = random.sample(range(original_space.shape[0]), num_points)\n",
    "\n",
    "    # Calculate pairwise distances in both spaces\n",
    "    selected_original = original_space[indices]\n",
    "    selected_reduced = reduced_space[indices]\n",
    "\n",
    "    # Compute pairwise distances from selected points to all points\n",
    "    dist_original = distance.cdist(selected_original, original_space)\n",
    "    dist_reduced = distance.cdist(selected_reduced, reduced_space)\n",
    "\n",
    "    # dist_original = distance.squareform(distance.pdist(original_space))\n",
    "    # dist_reduced = distance.squareform(distance.pdist(reduced_space))\n",
    "\n",
    "\n",
    "    \n",
    "    # # Ranking distances for each point\n",
    "    # ranks_original = np.argsort(dist_original, axis=1)[:, 1:k+1]\n",
    "    # ranks_reduced = np.argsort(dist_reduced, axis=1)[:, 1:k+1]\n",
    "\n",
    "    # # Calculate LCMC\n",
    "    # correlations = []\n",
    "    # for i in range(original_space.shape[0]):\n",
    "    #     # Spearman rank correlation for the k-nearest neighbors\n",
    "    #     rank_corr, _ = spearmanr(ranks_original[i], ranks_reduced[i])\n",
    "    #     correlations.append(rank_corr)\n",
    "\n",
    "    correlations = []\n",
    "    for i in range(len(indices)):\n",
    "        # Get ranks of the k nearest neighbors\n",
    "        neighbors_original = np.argsort(dist_original[i])[:k + 1]\n",
    "        neighbors_reduced = np.argsort(dist_reduced[i])[:k + 1]\n",
    "\n",
    "        # Spearman rank correlation\n",
    "        rank_corr, _ = spearmanr(neighbors_original, neighbors_reduced)\n",
    "        correlations.append(rank_corr)\n",
    "    \n",
    "    lcmc_score = np.nanmean(correlations)\n",
    "    return lcmc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec212b7-b9a0-4a0b-889d-4eac099ef883",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name in ['COIL-20','COIL-100']:\n",
    "    data = np.load(f'data/{dataset_name}/prepared/data.npy')\n",
    "    data = data.reshape(len(data), -1)\n",
    "else:\n",
    "    data = np.load(f'data/{dataset_name}/prepared/train_data.npy')\n",
    "    data = data.reshape(len(data), -1)\n",
    "data = data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5fbd5-a8d5-4f61-b71d-255deb58b502",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    try:\n",
    "        latent_data = np.load(f'data/{dataset_name}/{model_name}_latent_output_{version}.npy')\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    print(data.shape)\n",
    "    print(latent_data.shape)\n",
    "    lcmc_score = calculate_lcmc(data, latent_data, k=30)\n",
    "    print(f\"Model: {model_name}, LCMC Score: {lcmc_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e2cc6-94f6-4aea-beb2-d3e07209fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# original_space = np.array([...]) # Your high-dimensional data\n",
    "# reduced_space = np.array([...])  # Your reduced-dimensional data\n",
    "# k = 5 # Number of nearest neighbors\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
