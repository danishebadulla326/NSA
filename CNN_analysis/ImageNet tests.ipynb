{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd07d5-3623-4c8d-ba94-d2613170965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7570b-7018-4a91-b8a0-c2289031ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ac1bd6-f9d9-475f-bfd4-fe8529eaac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a6270f-59df-4091-8637-54f133401cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del features\n",
    "features = []\n",
    "\n",
    "def hook(module, input, output):\n",
    "    features.append(output)\n",
    "\n",
    "# Assuming you want to extract features after each of the four main blocks\n",
    "layers = [\n",
    "    resnet18.layer1,\n",
    "    resnet18.layer1[0],\n",
    "    resnet18.layer2,\n",
    "    resnet18.layer2[0],\n",
    "    resnet18.layer3,\n",
    "    resnet18.layer3[0],\n",
    "    resnet18.layer4,\n",
    "    resnet18.layer4[0],\n",
    "    resnet18.fc\n",
    "]\n",
    "\n",
    "for layer in layers:\n",
    "    layer.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32126d-8eb8-478c-b41e-ee5bec1472de",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c06536-afc7-4a6a-ac75-662994eef409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "final_layer1_b1 = torch.empty((0, 64, 56, 56))\n",
    "final_layer2_b1 = torch.empty((0, 128, 28, 28))\n",
    "final_layer3_b1 = torch.empty((0, 256, 14, 14))\n",
    "final_layer4_b1 = torch.empty((0, 512, 7, 7))\n",
    "\n",
    "final_layer1_b2 = torch.empty((0, 64, 56, 56))\n",
    "final_layer2_b2 = torch.empty((0, 128, 28, 28))\n",
    "final_layer3_b2 = torch.empty((0, 256, 14, 14))\n",
    "final_layer4_b2 = torch.empty((0, 512, 7, 7))\n",
    "\n",
    "final_fc = torch.empty((0,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd864b3c-9558-4462-9075-cd18a0b67d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vals = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d62db4-1c4d-4406-a44c-6299cb0912e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d3397-5767-48de-9196-7eeeb91a5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670259a3-b7ac-4cf9-add4-4596b1f8634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f772d6-dcbf-41f5-bef0-b3d0d9d8bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "import opendatasets as od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b85df-2274-4991-8c11-3d909c5a7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c imagenet-object-localization-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f354ea-816d-4dca-8b25-5fffd5d3c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "class FlatDirectoryImages(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = [f for f in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.images[idx])\n",
    "        # Using PIL Image to ensure compatibility with torchvision transforms\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "# Define your transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = FlatDirectoryImages(root_dir='data/ILSVRC/Data/CLS-LOC/val', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d63dbb-9ca7-4d1c-a5da-b57bb1693e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b60f0-854c-4e5c-bf67-d886a4c5cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=256, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fbe40f-4521-4bdc-9910-d272900842d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet18.to('cuda')\n",
    "features=[]\n",
    "resnet18.eval()\n",
    "# Don't forget to move your input to the same device as your model\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i,images in enumerate(data_loader):\n",
    "        del features\n",
    "        features=[]\n",
    "        images = images.to('cuda')\n",
    "        print(i)\n",
    "        outputs = resnet18(images)\n",
    "        print(len(features))\n",
    "        final_layer1_b1 = torch.cat((final_layer1_b1,features[0].cpu()), dim=0)\n",
    "        final_layer2_b1 = torch.cat((final_layer2_b1,features[2].cpu()), dim=0)\n",
    "        final_layer3_b1 = torch.cat((final_layer3_b1,features[4].cpu()), dim=0)\n",
    "        final_layer4_b1 = torch.cat((final_layer4_b1,features[6].cpu()), dim=0)\n",
    "        \n",
    "        final_layer1_b2 = torch.cat((final_layer1_b2,features[1].cpu()), dim=0)\n",
    "        final_layer2_b2 = torch.cat((final_layer2_b2,features[3].cpu()), dim=0)\n",
    "        final_layer3_b2 = torch.cat((final_layer3_b2,features[5].cpu()), dim=0)\n",
    "        final_layer4_b2 = torch.cat((final_layer4_b2,features[7].cpu()), dim=0)\n",
    "        \n",
    "        final_fc = torch.cat((final_fc,features[8].cpu()), dim=0)\n",
    "        torch.cuda.empty_cache()\n",
    "        features.clear()\n",
    "        # if i==20:\n",
    "        #     break\n",
    "        #torch.cuda.empty_cache()\n",
    "        #del images\n",
    "        # Your code to handle the outputs goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f96e066-1c81-4b93-9484-be205e964bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_layer1_b1.shape)\n",
    "print(final_layer2_b1.shape)\n",
    "print(final_layer3_b1.shape)\n",
    "print(final_layer4_b1.shape)\n",
    "print(final_layer1_b2.shape)\n",
    "print(final_layer2_b2.shape)\n",
    "print(final_layer3_b2.shape)\n",
    "print(final_layer4_b2.shape)\n",
    "print(final_fc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e4e72c-da7c-475f-bd74-5dc62242f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_layer1_b1,'output/final_layer1_b1.pt')\n",
    "torch.save(final_layer2_b1,'output/final_layer2_b1.pt')\n",
    "torch.save(final_layer3_b1,'output/final_layer3_b1.pt')\n",
    "torch.save(final_layer4_b1,'output/final_layer4_b1.pt')\n",
    "torch.save(final_layer1_b2,'output/final_layer1_b2.pt')\n",
    "torch.save(final_layer2_b2,'output/final_layer2_b2.pt')\n",
    "torch.save(final_layer3_b2,'output/final_layer3_b2.pt')\n",
    "torch.save(final_layer4_b2,'output/final_layer4_b2.pt')\n",
    "torch.save(final_fc,'output/final_fc.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
